{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import wget\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://rgai.inf.u-szeged.hu/~berend/compsem/en-cbow.vec.gz\"\n",
    "embedding_file_name = url.split('/')[-1]\n",
    "if not os.path.exists(embedding_file_name):\n",
    "    filename = wget.download(url)\n",
    "    print(filename, \" got downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "50000\n",
      "75000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "#load embeddings\n",
    "dimensions=-1\n",
    "w2i={}\n",
    "i2w={}\n",
    "embeddings = []\n",
    "j=0\n",
    "for i,l in enumerate(gzip.open(embedding_file_name, 'rt')):\n",
    "    parts=l.strip().split()\n",
    "    if i==0:\n",
    "        dimensions=int(parts[1])\n",
    "        continue\n",
    "    vector=list(map(float, parts[1:]))\n",
    "    if np.linalg.norm(vector) > 0:\n",
    "        i2w[j]=parts[0]\n",
    "        w2i[parts[0]]=j\n",
    "        embeddings.append(vector)\n",
    "        j+=1\n",
    "    if i>0 and i%25000==0: print(i)\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(M):\n",
    "    row_sums = np.linalg.norm(M, axis=1) + 1e-9\n",
    "    return M / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Modify the code below to calculate the empirical unigram frequencies from our corpus and the appropriate weighting factor derived from it according to the SIF algorithm (using a=1e-4)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0543296045209654e-05 1.0543296045209654e-05\n",
      "0.0002858226756120503 0.0251731825575413\n",
      "0.2591864250626661 0.00395676325181139\n"
     ]
    }
   ],
   "source": [
    "#obtain sentences and words of the desired corpus first\n",
    "corpus_file='austen-emma.txt'\n",
    "sentences=gutenberg.sents(corpus_file)\n",
    "\n",
    "words = gutenberg.words(corpus_file)\n",
    "freq = collections.Counter(words)\n",
    "empirical_unigram_freq = {w:f/len(words) for w,f in freq.items()}\n",
    "\n",
    "a=1e-4\n",
    "word_weights = {w:a/(a+p) for w,p in empirical_unigram_freq.items()}\n",
    "\n",
    "uniform_unigram_freq = {w: 1/len(w2i) for w in w2i}\n",
    "print(uniform_unigram_freq['walk'], uniform_unigram_freq['the'])\n",
    "print(empirical_unigram_freq['walk'], empirical_unigram_freq['the'])\n",
    "print(word_weights['walk'], word_weights['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sentence_i(sents, i):\n",
    "    return ' '.join(sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" You have made her too tall , Emma ,\" said Mr . Knightley .\n"
     ]
    }
   ],
   "source": [
    "print(select_sentence_i(sentences, 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_vector(tokens, unigram_weights):\n",
    "    weighted_sum=np.zeros(64)\n",
    "    normalizer=1e-15 # avoid dividing by 0\n",
    "    for t in tokens:\n",
    "        if t in w2i:\n",
    "            normalizer+=1\n",
    "            weighted_sum += unigram_weights[t] * embeddings[w2i[t]]\n",
    "    return weighted_sum/normalizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = np.array([create_sentence_vector(s, word_weights) for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_vector(M, v, k=5):\n",
    "    row_normalized = normalize_matrix(M)\n",
    "    similarities = row_normalized @ v\n",
    "    return similarities, np.argsort(similarities)[-k:]\n",
    "\n",
    "def get_most_similar_sentence(sentence_embeddings, i, k=5):\n",
    "    return get_most_similar_vector(sentence_embeddings, sentence_embeddings[i], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I only doubt whether he will ever take us anywhere else . \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: I only doubt whether he will ever take us anywhere else .\n",
      "Sentence with top-2 similarity: I wish I were anywhere else .\"\n",
      "Sentence with top-3 similarity: one can think of nothing else .\"\n",
      "=============*****=============\n",
      "Pray , pray attempt it . \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: Pray , pray attempt it .\n",
      "Sentence with top-2 similarity: No -- pray let her have time to look about her .\"\n",
      "Sentence with top-3 similarity: Come , sir , pray let me hear it .\"\n",
      "=============*****=============\n",
      "How cheerful , how animated , how suspicious , how busy their imaginations all are !\" \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: How cheerful , how animated , how suspicious , how busy their imaginations all are !\"\n",
      "Sentence with top-2 similarity: I saw the word , and am curious to know how it could be so very entertaining to the one , and so very distressing to the other .\"\n",
      "Sentence with top-3 similarity: He found he could not be useful , and his feelings were too much irritated for talking .\n",
      "=============*****=============\n",
      "\" What a very great pleasure it will be to you ! \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: \" What a very great pleasure it will be to you !\n",
      "Sentence with top-2 similarity: Yes , that will be quite enough for pleasure .\n",
      "Sentence with top-3 similarity: What a pity it is that Mr . Weston ever thought of her !\"\n",
      "=============*****=============\n"
     ]
    }
   ],
   "source": [
    "for test_sentence in [55, 627, 880, 1928]:\n",
    "    print(select_sentence_i(sentences, test_sentence), \"\\n\", ''.join(50*['-']))\n",
    "    similarities, argsort = get_most_similar_sentence(sentence_vectors, test_sentence, 3)\n",
    "    for k, top_id in enumerate(argsort[::-1]):\n",
    "        print(\"Sentence with top-{} similarity: {}\".format(k+1, select_sentence_i(sentences, top_id)))\n",
    "    print(\"=============*****=============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence vectors share a fair amount of directions purely due to function words and stop words.\n",
    "We can discard this component of the representations by removing the sentence vectors to the principal singular vector $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1) (7752, 64)\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=1, random_state=0)\n",
    "svd.fit(sentence_vectors)\n",
    "u=np.array(svd.components_.T)\n",
    "print(u.shape, sentence_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal vector $u \\in R^d$ can be regarded as a word vector itself. We can query those words which have the most similar vectorial representation and see if the principal singular vector indeed encodes grammatical information mostly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words with highest similarity to the principal singular vector are:  ['merely', 'if', 'because', 'so', 'even']\n"
     ]
    }
   ],
   "source": [
    "word_similarities, ordering = get_most_similar_vector(embeddings, svd.components_[0])\n",
    "print(\"The words with highest similarity to the principal singular vector are: \", [i2w[o] for o in ordering])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Modify the code which looks for the most similar sentences, but this time also consider the correction term involving the projection of the sentence vectors to the principal singular vector $u$ in the SIF algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I only doubt whether he will ever take us anywhere else . \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: I only doubt whether he will ever take us anywhere else .\n",
      "Sentence with top-2 similarity: I wish I were anywhere else .\"\n",
      "Sentence with top-3 similarity: one can think of nothing else .\"\n",
      "=============*****=============\n",
      "Pray , pray attempt it . \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: Pray , pray attempt it .\n",
      "Sentence with top-2 similarity: Come , sir , pray let me hear it .\"\n",
      "Sentence with top-3 similarity: No -- pray let her have time to look about her .\"\n",
      "=============*****=============\n",
      "How cheerful , how animated , how suspicious , how busy their imaginations all are !\" \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: How cheerful , how animated , how suspicious , how busy their imaginations all are !\"\n",
      "Sentence with top-2 similarity: It was a happy circumstance , and animated Mr . Woodhouse for some time .\n",
      "Sentence with top-3 similarity: I saw the word , and am curious to know how it could be so very entertaining to the one , and so very distressing to the other .\"\n",
      "=============*****=============\n",
      "I dare say we shall be all safe at Hartfield before midnight .\" \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: I dare say we shall be all safe at Hartfield before midnight .\"\n",
      "Sentence with top-2 similarity: I think it would be much better if they would come in one afternoon next summer , and take their tea with us -- take us in their afternoon walk ; which they might do , as our hours are so reasonable , and yet get home without being out in the damp of the evening .\n",
      "Sentence with top-3 similarity: \" And so she is to come to us next Friday or Saturday , and the Campbells leave town in their way to Holyhead the Monday following -- as you will find from Jane ' s letter .\n",
      "=============*****=============\n"
     ]
    }
   ],
   "source": [
    "for test_sentence in [55, 627, 880, 2020]:\n",
    "    print(select_sentence_i(sentences, test_sentence), \"\\n\", ''.join(50*['-']))\n",
    "    corrected_sentence_vectors = sentence_vectors - sentence_vectors @ u @ u.T\n",
    "    similarities, argsort = get_most_similar_sentence(corrected_sentence_vectors, test_sentence, 3)\n",
    "    for k, top_id in enumerate(argsort[::-1]):\n",
    "        print(\"Sentence with top-{} similarity: {}\".format(k+1, select_sentence_i(sentences, top_id)))\n",
    "    print(\"=============*****=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/berend/anaconda2/envs/py3k/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: nltk in /home/berend/anaconda2/envs/py3k/lib/python3.7/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /home/berend/anaconda2/envs/py3k/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: numpy in /home/berend/anaconda2/envs/py3k/lib/python3.7/site-packages (1.17.4)\n",
      "Requirement already satisfied: sklearn in /home/berend/anaconda2/envs/py3k/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/berend/anaconda2/envs/py3k/lib/python3.7/site-packages (from sklearn) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/berend/anaconda2/envs/py3k/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.17.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/berend/anaconda2/envs/py3k/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/berend/anaconda2/envs/py3k/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import wget\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://rgai1.inf.u-szeged.hu/~berend/compsem/en-cbow.vec.gz\"\n",
    "embedding_file_name = url.split('/')[-1]\n",
    "if not os.path.exists(embedding_file_name):\n",
    "    filename = wget.download(url)\n",
    "    print(filename, \" got downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "50000\n",
      "75000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "#load embeddings\n",
    "dimensions=-1\n",
    "w2i={}\n",
    "i2w={}\n",
    "embeddings = []\n",
    "j=0\n",
    "for i,l in enumerate(gzip.open(embedding_file_name, 'rt')):\n",
    "    parts=l.strip().split()\n",
    "    if i==0:\n",
    "        dimensions=int(parts[1])\n",
    "        continue\n",
    "    vector=list(map(float, parts[1:]))\n",
    "    if np.linalg.norm(vector) > 0:\n",
    "        i2w[j]=parts[0]\n",
    "        w2i[parts[0]]=j\n",
    "        embeddings.append(vector)\n",
    "        j+=1\n",
    "    if i>0 and i%25000==0: print(i)\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(M):\n",
    "    row_sums = np.linalg.norm(M, axis=1) + 1e-9\n",
    "    return M / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Modify the code below to calculate the empirical unigram frequencies from our corpus and the appropriate weighting factor derived from it according to the SIF algorithm (using a=1e-4)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0543296045209654e-05 1.0543296045209654e-05\n",
      "1.0543296045209654e-05 1.0543296045209654e-05\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "#obtain sentences and words of the desired corpus first\n",
    "corpus_file='austen-emma.txt'\n",
    "sentences=gutenberg.sents(corpus_file)\n",
    "\n",
    "uniform_unigram_freq = {w: 1/len(w2i) for w in w2i}\n",
    "\n",
    "words = gutenberg.words(corpus_file)\n",
    "freq = collections.Counter(words)\n",
    "### MODIFY CODE BELOW ####\n",
    "empirical_unigram_freq = uniform_unigram_freq\n",
    "\n",
    "a=1e-4\n",
    "word_weights = {w:1 for w,p in empirical_unigram_freq.items()}\n",
    "\n",
    "print(uniform_unigram_freq['walk'], uniform_unigram_freq['the'])\n",
    "print(empirical_unigram_freq['walk'], empirical_unigram_freq['the'])\n",
    "print(word_weights['walk'], word_weights['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sentence_i(sents, i):\n",
    "    return ' '.join(sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" You have made her too tall , Emma ,\" said Mr . Knightley .\n"
     ]
    }
   ],
   "source": [
    "print(select_sentence_i(sentences, 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_vector(tokens, unigram_weights):\n",
    "    weighted_sum=np.zeros(64)\n",
    "    normalizer=1e-15 # avoid dividing by 0\n",
    "    for t in tokens:\n",
    "        if t in w2i:\n",
    "            normalizer+=1\n",
    "            weighted_sum += unigram_weights[t] * embeddings[w2i[t]]\n",
    "    return weighted_sum/normalizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = np.array([create_sentence_vector(s, word_weights) for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_vector(M, v, k=5):\n",
    "    row_normalized = normalize_matrix(M)\n",
    "    similarities = row_normalized @ v\n",
    "    return similarities, np.argsort(similarities)[-k:]\n",
    "\n",
    "def get_most_similar_sentence(sentence_embeddings, i, k=5):\n",
    "    return get_most_similar_vector(sentence_embeddings, sentence_embeddings[i], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I only doubt whether he will ever take us anywhere else . \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: I only doubt whether he will ever take us anywhere else .\n",
      "Sentence with top-2 similarity: \" Certainly , if you wish it ;-- but you are not going to walk to Highbury alone ?\"\n",
      "Sentence with top-3 similarity: And when you get there , you must tell him at what time you would have him come for you again ; and you had better name an early hour .\n",
      "=============*****=============\n",
      "Pray , pray attempt it . \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: Pray , pray attempt it .\n",
      "Sentence with top-2 similarity: I long to make apologies , excuses , to urge something for myself .\n",
      "Sentence with top-3 similarity: \" I was just going to tell you of our agreeable surprize in seeing him arrive this morning .\n",
      "=============*****=============\n",
      "How cheerful , how animated , how suspicious , how busy their imaginations all are !\" \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: How cheerful , how animated , how suspicious , how busy their imaginations all are !\"\n",
      "Sentence with top-2 similarity: How inconsiderate , how indelicate , how irrational , how unfeeling had been her conduct !\n",
      "Sentence with top-3 similarity: The tone implied some old acquaintance -- and how could she possibly guess ?\n",
      "=============*****=============\n",
      "I dare say we shall be all safe at Hartfield before midnight .\" \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: I dare say we shall be all safe at Hartfield before midnight .\"\n",
      "Sentence with top-2 similarity: Till they all come in I shall not be missed ; and when they do , will you have the goodness to say that I am gone ?\"\n",
      "Sentence with top-3 similarity: \" I must spend some time with them ; I am sure they will want it ;-- afterwards I may probably be glad to dispose of myself .\n",
      "=============*****=============\n"
     ]
    }
   ],
   "source": [
    "for test_sentence in [55, 627, 880, 2020]:\n",
    "    print(select_sentence_i(sentences, test_sentence), \"\\n\", ''.join(50*['-']))\n",
    "    similarities, argsort = get_most_similar_sentence(sentence_vectors, test_sentence, 3)\n",
    "    for k, top_id in enumerate(argsort[::-1]):\n",
    "        print(\"Sentence with top-{} similarity: {}\".format(k+1, select_sentence_i(sentences, top_id)))\n",
    "    print(\"=============*****=============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence vectors share a fair amount of directions purely due to function words and stop words.\n",
    "We can discard this component of the representations by removing the sentence vectors to the principal singular vector $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1) (7752, 64)\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=1, random_state=0)\n",
    "svd.fit(sentence_vectors)\n",
    "u=np.array(svd.components_.T)\n",
    "print(u.shape, sentence_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal vector $u \\in R^d$ can be regarded as a word vector itself. We can query those words which have the most similar vectorial representation and see if the principal singular vector indeed encodes grammatical information mostly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words with highest similarity to the principal singular vector are:  ['actually', 'so', 'if', 'that', 'but']\n"
     ]
    }
   ],
   "source": [
    "word_similarities, ordering = get_most_similar_vector(embeddings, svd.components_[0])\n",
    "print(\"The words with highest similarity to the principal singular vector are: \", [i2w[o] for o in ordering])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Modify the code which looks for the most similar sentences, but this time also consider the correction term involving the projection of the sentence vectors to the principal singular vector $u$ in the SIF algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I only doubt whether he will ever take us anywhere else . \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: I only doubt whether he will ever take us anywhere else .\n",
      "Sentence with top-2 similarity: \" Certainly , if you wish it ;-- but you are not going to walk to Highbury alone ?\"\n",
      "Sentence with top-3 similarity: And when you get there , you must tell him at what time you would have him come for you again ; and you had better name an early hour .\n",
      "=============*****=============\n",
      "Pray , pray attempt it . \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: Pray , pray attempt it .\n",
      "Sentence with top-2 similarity: I long to make apologies , excuses , to urge something for myself .\n",
      "Sentence with top-3 similarity: \" I was just going to tell you of our agreeable surprize in seeing him arrive this morning .\n",
      "=============*****=============\n",
      "How cheerful , how animated , how suspicious , how busy their imaginations all are !\" \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: How cheerful , how animated , how suspicious , how busy their imaginations all are !\"\n",
      "Sentence with top-2 similarity: How inconsiderate , how indelicate , how irrational , how unfeeling had been her conduct !\n",
      "Sentence with top-3 similarity: The tone implied some old acquaintance -- and how could she possibly guess ?\n",
      "=============*****=============\n",
      "I dare say we shall be all safe at Hartfield before midnight .\" \n",
      " --------------------------------------------------\n",
      "Sentence with top-1 similarity: I dare say we shall be all safe at Hartfield before midnight .\"\n",
      "Sentence with top-2 similarity: Till they all come in I shall not be missed ; and when they do , will you have the goodness to say that I am gone ?\"\n",
      "Sentence with top-3 similarity: \" I must spend some time with them ; I am sure they will want it ;-- afterwards I may probably be glad to dispose of myself .\n",
      "=============*****=============\n"
     ]
    }
   ],
   "source": [
    "for test_sentence in [55, 627, 880, 2020]:\n",
    "    print(select_sentence_i(sentences, test_sentence), \"\\n\", ''.join(50*['-']))\n",
    "    corrected_sentence_vectors = sentence_vectors  ### CHANGE THIS LINE ###\n",
    "    similarities, argsort = get_most_similar_sentence(corrected_sentence_vectors, test_sentence, 3)\n",
    "    for k, top_id in enumerate(argsort[::-1]):\n",
    "        print(\"Sentence with top-{} similarity: {}\".format(k+1, select_sentence_i(sentences, top_id)))\n",
    "    print(\"=============*****=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

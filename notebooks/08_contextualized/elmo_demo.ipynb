{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import numpy as np\n",
    "\n",
    "from allennlp.modules.elmo import batch_to_ids\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained Elmo-style contextualized embeddings can be downloaded from the [official website](https://allennlp.org/elmo). There are models of different sizes ranging from roughly $10^7$ parameters to approximately $10^8$ parameters.  For simplicity, here we will rely on the smallest (hence fastes to download and least accurate) model with 1024 dimensional hidden vector and 128 dimensional output vector size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_url = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5'\n",
    "options_url = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json'\n",
    "weight_file = weight_url.split('/')[-1]\n",
    "options_file = options_url.split('/')[-1]\n",
    "\n",
    "if not os.path.exists(weight_file):\n",
    "    wget.download(weight_url)\n",
    "if not os.path.exists(options_file):\n",
    "    wget.download(options_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `batch_to_ids` method transforms a list of tokenized list of sentences into a Tensor of encoded characters of size (number of sentences, number of longest sentence, length of token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 50])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [['I', 'was', 'sitting', 'at', 'the', 'bank', 'of', 'the', 'river', '.'],\n",
    "             ['That', 'bank', 'was', 'robbed', 'recently', '.'],\n",
    "             ['I', 'saw', 'a', 'bug', 'flying', 'next', 'to', 'the', 'water', '.'],\n",
    "             ['The', 'quick', 'brown', 'fox', 'ate', 'a', 'chicken', '.'],\n",
    "             ['The', 'lazy', 'dog', 'visits', 'the', 'bank', '.']]\n",
    "character_ids = batch_to_ids(sentences)\n",
    "character_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the biLSTM based on the previously downloaded pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/15/2019 21:02:38 - INFO - allennlp.commands.elmo -   Initializing ELMo.\n"
     ]
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder(options_file, weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = elmo.embed_sentence(\"The bank is located at the bank of Thames .\".split(' '))\n",
    "vectorsB = elmo.embed_sentence(\"The bank is closed on Sundays .\".split(' '))\n",
    "vectorsC = elmo.embed_sentence(\"Can you see that nice swan at the river bank ?\".split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ElmoEmbedder` encodes each sentence into a `(3, token number, embedding size)` tensor. Note that the input layer comes from character level convolutions, which are then fed as inputs into a 2 layer bi-LSTM. The biLSTM outputs a `2*hidden dimension` sized vector for each token position and layer.  \n",
    "The bidirectional language model looks the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ElmoBiLm(\n",
      "  (_token_embedder): _ElmoCharacterEncoder(\n",
      "    (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
      "    (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
      "    (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
      "    (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
      "    (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
      "    (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
      "    (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
      "    (_highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (_projection): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  )\n",
      "  (_elmo_lstm): ElmoLstm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=128, out_features=4096, bias=False)\n",
      "      (state_linearity): Linear(in_features=128, out_features=4096, bias=True)\n",
      "      (state_projection): Linear(in_features=1024, out_features=128, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=128, out_features=4096, bias=False)\n",
      "      (state_linearity): Linear(in_features=128, out_features=4096, bias=True)\n",
      "      (state_projection): Linear(in_features=1024, out_features=128, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=128, out_features=4096, bias=False)\n",
      "      (state_linearity): Linear(in_features=128, out_features=4096, bias=True)\n",
      "      (state_projection): Linear(in_features=1024, out_features=128, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=128, out_features=4096, bias=False)\n",
      "      (state_linearity): Linear(in_features=128, out_features=4096, bias=True)\n",
      "      (state_projection): Linear(in_features=1024, out_features=128, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(elmo.elmo_bilm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the vectorial representations of the same word `bank` in different context across multiple mentions and layers of the biLSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "LAYER 0\n",
      "[[0.         0.30206925 0.01709425 0.23226154]\n",
      " [0.30206925 0.         0.3021521  0.24452692]\n",
      " [0.01709425 0.3021521  0.         0.21635085]\n",
      " [0.23226154 0.24452692 0.21635085 0.        ]]\n",
      "LAYER 1\n",
      "[[0.         0.42234105 0.06816322 0.43650234]\n",
      " [0.42234105 0.         0.41525936 0.44247025]\n",
      " [0.06816322 0.41525936 0.         0.39149922]\n",
      " [0.43650234 0.44247025 0.39149922 0.        ]]\n",
      "LAYER 2\n"
     ]
    }
   ],
   "source": [
    "for layer in range(3):\n",
    "    bank_vectors = np.array([vectors[layer][1], vectors[layer][6], vectorsB[layer][1], vectorsC[layer][9]])\n",
    "    print(cosine_distances(bank_vectors))\n",
    "    print('LAYER {}'.format(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the representations at Layer 0 for the same word are identical as representations at that layer merely come from the character-level convolutional part of the network with no context-awareness.  \n",
    "Representations for the same word at layer 1 and 2 differ however. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo for many languages\n",
    "\n",
    "The [ElmoForManyLangs](https://github.com/HIT-SCIR/ELMoForManyLangs) project provides pretrained ELMo-style embeddings for multiple languages and utils to use them.  \n",
    "Once installed, one has to provide the folder containing the model files during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/15/2019 21:03:18 - INFO - root -   char embedding size: 3407\n",
      "04/15/2019 21:03:22 - INFO - root -   word embedding size: 343569\n",
      "04/15/2019 21:03:30 - INFO - root -   Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(343569, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(3407, 50, padding_idx=3404)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from elmoformanylangs import Embedder\n",
    "e = Embedder('./hun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/15/2019 21:08:02 - INFO - root -   1 batches, avg len: 11.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mondatok = [\"Ebben a várban sok katona megfordult a középkor során .\".split(),\n",
    "            \"Mária megunta a várakozást , és nem vár tovább .\".split(),\n",
    "            \"A végvárakban sok derék katona lelte halálát .\".split()\n",
    "           ]\n",
    "vectors = e.sents2elmo(mondatok, output_layer=-2)\n",
    "vectors[0].shape\n",
    "vectors[0][layer][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1024)\n",
      "[[0.         0.52950865 0.41278762]\n",
      " [0.52950865 0.         0.6484362 ]\n",
      " [0.41278762 0.6484362  0.        ]]\n",
      "LAYER 0\n",
      "(3, 1024)\n",
      "[[0.         0.7263053  0.4613278 ]\n",
      " [0.7263053  0.         0.81253874]\n",
      " [0.4613278  0.81253874 0.        ]]\n",
      "LAYER 1\n",
      "(3, 1024)\n",
      "[[0.         0.8110876  0.42670667]\n",
      " [0.8110876  0.         0.87850004]\n",
      " [0.42670667 0.87850004 0.        ]]\n",
      "LAYER 2\n"
     ]
    }
   ],
   "source": [
    "for layer in range(3):\n",
    "    var_vectors = np.array([vectors[0][layer][2], vectors[1][layer][7], vectors[2][layer][1]])\n",
    "    print(var_vectors.shape)\n",
    "    print(cosine_distances(var_vectors))\n",
    "    print('LAYER {}'.format(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

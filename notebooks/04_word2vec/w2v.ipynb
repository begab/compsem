{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some corpora to work with first\n",
    "\n",
    "This is just the usual way to load some text corpus from the Gutenberg Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpora consist of these files:\n",
      " ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "print(\"The corpora consist of these files:\\n\", gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go for two different corpora, i.e. the Bible (King James Version) and Shakespeare's Hamlet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_text = nltk.Text(gutenberg.sents('bible-kjv.txt'))\n",
    "hamlet_text = nltk.Text(gutenberg.sents('shakespeare-hamlet.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train word2vec word embeddings\n",
    "\n",
    "We can train [word2vec](https://arxiv.org/abs/1301.3781) embeddings using the [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) class from the `gensim` library by simply providing it with a list of sentences. Note that passing a list of sentences is a pretty bad idea for enourmous datasets, as this approach would assume that the entire corpus is stored in the main memory. An alternative solution would be to pass a data stream as we are training word2vec using stochastic gradient descent after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most similar words to the word 'daughter' according to the Bible corpus are\n",
      "1 ('Shem', 0.37578085064888)\n",
      "2 ('cruel', 0.3495354950428009)\n",
      "3 ('advantage', 0.3479419946670532)\n",
      "4 ('Kithlish', 0.34496957063674927)\n",
      "5 ('cornfloor', 0.33982983231544495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berend/anaconda2/envs/py3k/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 5 most similar words to the word 'daughter' according to the Hamlet corpus are\n",
      "1 ('suiting', 0.38659748435020447)\n",
      "2 ('Saue', 0.3700949251651764)\n",
      "3 ('shoulder', 0.3136047422885895)\n",
      "4 ('pastime', 0.3134770691394806)\n",
      "5 ('Nose', 0.3118637204170227)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "bible_model = Word2Vec(min_count=1)\n",
    "bible_model.build_vocab(bible_text)\n",
    "\n",
    "top=5\n",
    "word = \"daughter\"\n",
    "print(\"The {} most similar words to the word '{}' according to the Bible corpus are\".format(top, word))\n",
    "for i, neighbor in enumerate(bible_model.wv.most_similar(word, topn=top)):\n",
    "    print(i+1, neighbor)\n",
    "\n",
    "hamlet_model = Word2Vec(min_count=1)\n",
    "hamlet_model.build_vocab(hamlet_text)\n",
    "if word in hamlet_model.wv.vocab:\n",
    "    print(\"\\nThe {} most similar words to the word '{}' according to the Hamlet corpus are\".format(top, word))\n",
    "    for i, neighbor in enumerate(hamlet_model.wv.most_similar(word, topn=top)):\n",
    "        print(i+1, neighbor)\n",
    "else:\n",
    "    print(\"Word '%s' not found in vocabulary.\" % word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seem moderately impressive, we could say. Haven't we maybe forgot about something? Perhaps we should train our model for a few epochs first before querying it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30103 1010654\n",
      "The 5 most similar words to the word 'daughter' according to the Bible corpus are\n",
      "1 ('brother', 0.9469287991523743)\n",
      "2 ('ruler', 0.9367074966430664)\n",
      "3 ('beauty', 0.9359796643257141)\n",
      "4 ('bore', 0.9346645474433899)\n",
      "5 ('Amorite', 0.9309505224227905)\n"
     ]
    }
   ],
   "source": [
    "print(bible_model.corpus_count, bible_model.corpus_total_words)\n",
    "bible_model.train(bible_text, total_examples=bible_model.corpus_count, epochs=2)\n",
    "print(\"The {} most similar words to the word '{}' according to the Bible corpus are\".format(top, word))\n",
    "for i, neighbor in enumerate(bible_model.wv.most_similar(word, topn=top)):\n",
    "    print(i+1, neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since we were not stating it otherwise, the default model, meaning CBOW (`sg=0`) with negative sampling (`ns=5`) was trained window size of 5.\n",
    "\n",
    "It is a good exercise to spend some time to see the effects of changing these hyperpameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting phrases\n",
    "\n",
    "One limitation of word2vec is that it determines vectors to simple word forms and ignores larger semantic units such as noun phrases, such as _white wine_ or _white rabbit_.\n",
    "\n",
    "We can mitigate this problem if we first perform a preprocessing step, which combines frequently co-occurring sequences of terms based on some statistics such as the (normalized) [pointwise mutual information (PMI) score](https://en.wikipedia.org/wiki/Pointwise_mutual_information). The more likely two events are not co-occurring purely just by chance, the higher value their PMI value will be.\n",
    "\n",
    "The `gensim.models.phrases.Phrases` class provides us convenient tools to extract such likely to be meaningful phrases from a sequence of sentences. We can also set the minimum frequency for an expression to have in order to treat it as a frequent collocation and impose a PMI threshold above which we would like to treat a pair of words as a single unit in the followings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "phrases = Phrases(bible_text, min_count=10, threshold=5)\n",
    "phraser = Phraser(phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the collocations found in the first 50 sentences of the Bible corpus using the above parametrization of the `Phrases` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'The First', 6.741405807651864)\n",
      "(b'1 In', 5.5260339133856915)\n",
      "(b'there was', 5.4156888352344215)\n",
      "(b'first day', 8.648403575989782)\n",
      "(b'gathered together', 92.87101866468568)\n",
      "(b'dry land', 16.398498089819473)\n",
      "(b'dry land', 16.398498089819473)\n",
      "(b'bring forth', 18.272821494060434)\n",
      "(b'his kind', 5.742911283376399)\n",
      "(b'brought forth', 13.936832424071403)\n",
      "(b'his kind', 5.742911283376399)\n",
      "(b'his kind', 5.742911283376399)\n",
      "(b'third day', 22.10716180371353)\n",
      "(b'rule over', 56.24873073088427)\n",
      "(b'bring forth', 18.272821494060434)\n",
      "(b'every living', 5.7755448529157)\n",
      "(b'brought forth', 13.936832424071403)\n",
      "(b'his kind', 5.742911283376399)\n",
      "(b'bring forth', 18.272821494060434)\n",
      "(b'living creature', 156.404410039878)\n",
      "(b'his kind', 5.742911283376399)\n",
      "(b'creeping thing', 42.49805596277646)\n",
      "(b'his kind', 5.742911283376399)\n",
      "(b'his kind', 5.742911283376399)\n",
      "(b'his kind', 5.742911283376399)\n",
      "(b'Let us', 14.426541438943472)\n",
      "(b'dominion over', 24.519752218753574)\n",
      "(b'creeping thing', 42.49805596277646)\n",
      "(b'his own', 8.238572394776547)\n",
      "(b'said unto', 7.621425663511157)\n",
      "(b'dominion over', 24.519752218753574)\n",
      "(b'every living', 5.7755448529157)\n",
      "(b'I have', 5.148694809798247)\n",
      "(b'shall be', 6.0869369745194835)\n",
      "(b'I have', 5.148694809798247)\n",
      "(b'every green', 10.353720650958632)\n",
      "(b'had made', 6.268833882518343)\n",
      "(b'seventh day', 33.529195402298846)\n",
      "(b'had made', 6.268833882518343)\n",
      "(b'seventh day', 33.529195402298846)\n",
      "(b'had made', 6.268833882518343)\n",
      "(b'seventh day', 33.529195402298846)\n",
      "(b'These are', 22.542108334143734)\n",
      "(b'there was', 5.4156888352344215)\n",
      "(b'went up', 8.539272209612504)\n",
      "(b'went out', 9.138575120776148)\n",
      "(b'from thence', 21.84155466273569)\n"
     ]
    }
   ],
   "source": [
    "for bigram in phrases.export_phrases(bible_text[0:50]):\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a default word2vec model yet another time, but this time focusing on bigrams as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_bible_model = Word2Vec(phraser[bible_text], min_count=1, iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Modify the below code such that it queries the top 10 most frequent words relative to the most frequently occurring bigram in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most similar words to the word 'very_much' according to the Bible corpus are\n",
      "1 ('wafers', 0.9597986936569214)\n",
      "2 ('innumerable', 0.9545854330062866)\n",
      "3 ('folk', 0.9539214968681335)\n",
      "4 ('unleavened', 0.9517351388931274)\n",
      "5 ('Ephrathites', 0.949032187461853)\n"
     ]
    }
   ],
   "source": [
    "bigram='very_much'\n",
    "for w, vocab_entry in bigram_bible_model.wv.vocab.items():\n",
    "    if '_' in w:\n",
    "        pass\n",
    "        \n",
    "print(\"The {} most similar words to the word '{}' according to the Bible corpus are\".format(top, bigram))\n",
    "for i, neighbor in enumerate(bigram_bible_model.wv.most_similar(bigram, topn=top)):\n",
    "    print(i+1, neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

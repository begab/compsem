{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
    "data_train = fetch_20newsgroups(categories=sport_categories, subset='train', shuffle=True, random_state=42)\n",
    "data_test = fetch_20newsgroups(categories=sport_categories, subset='test', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we downloaded a sample from the 20 news groups dataset as a `sklearn.utils.Bunch` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick peek into the data, i.e., check out the contents of the first train document and its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dougb@comm.mot.com (Doug Bank)\n",
      "Subject: Re: Info needed for Cleveland tickets\n",
      "Reply-To: dougb@ecs.comm.mot.com\n",
      "Organization: Motorola Land Mobile Products Sector\n",
      "Distribution: usa\n",
      "Nntp-Posting-Host: 145.1.146.35\n",
      "Lines: 17\n",
      "\n",
      "In article <1993Apr1.234031.4950@leland.Stanford.EDU>, bohnert@leland.Stanford.EDU (matthew bohnert) writes:\n",
      "\n",
      "|> I'm going to be in Cleveland Thursday, April 15 to Sunday, April 18.\n",
      "|> Does anybody know if the Tribe will be in town on those dates, and\n",
      "|> if so, who're they playing and if tickets are available?\n",
      "\n",
      "The tribe will be in town from April 16 to the 19th.\n",
      "There are ALWAYS tickets available! (Though they are playing Toronto,\n",
      "and many Toronto fans make the trip to Cleveland as it is easier to\n",
      "get tickets in Cleveland than in Toronto.  Either way, I seriously\n",
      "doubt they will sell out until the end of the season.)\n",
      "\n",
      "-- \n",
      "Doug Bank                       Private Systems Division\n",
      "dougb@ecs.comm.mot.com          Motorola Communications Sector\n",
      "dougb@nwu.edu                   Schaumburg, Illinois\n",
      "dougb@casbah.acns.nwu.edu       708-576-8207                    \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "print(data_train.data[0], data_train.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = sent_tokenize(data_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|> Does anybody know if the Tribe will be in town on those dates, and\n",
      "|> if so, who're they playing and if tickets are available?\n"
     ]
    }
   ],
   "source": [
    "print(sent_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['|', '>', 'Does', 'anybody', 'know', 'if', 'the', 'Tribe', 'will', 'be', 'in', 'town', 'on', 'those', 'dates', ',', 'and', '|', '>', 'if', 'so', ',', 'who', \"'re\", 'they', 'playing', 'and', 'if', 'tickets', 'are', 'available', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sent_text[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Figure out what are the top-10 most frequent words for the two document classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the term-document matrix\n",
    "\n",
    "We will use `sklearn.feature_extraction.text.{CountVectorizer,TfidfTransformer}` classes to create a term-document matrix representation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "get_tokens = lambda docs: [' '.join([t for sent in sent_tokenize(d) for t in word_tokenize(sent)]) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "train_unigram = vectorizer.fit_transform(get_tokens(data_train.data))\n",
    "# do not use fit_transform once more like that:\n",
    "# test_unigram_bad = vectorizer.fit_transform(get_tokens(data_test.data))\n",
    "# instead let's use just the transform method\n",
    "test_unigram = vectorizer.transform(get_tokens(data_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (1197, 18569) (796, 18569)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_unigram), train_unigram.shape, test_unigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '000256',\n",
       " '000th',\n",
       " '0010',\n",
       " '001211',\n",
       " '001323',\n",
       " '002',\n",
       " '002251w',\n",
       " '0023']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unigram.toarray()[0:3,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=3)\n",
    "bigram_vectorizer_memory_inefficient = CountVectorizer(ngram_range=(1, 2), min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let us repeat the same vectorization but this time use consecutive n-grams as well.\n",
    "In order to compansate for the increased amount of features, only keep ngrams that are present in at least 3 documents.\n",
    "\n",
    "1. How does the shape of the term-document matrix change?\n",
    "1. What are the first 10 features this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1197 vs 27960 (1197, 148870)\n"
     ]
    }
   ],
   "source": [
    "# Place your code here\n",
    "train_bigram_df_ge_3 = bigram_vectorizer.fit_transform(get_tokens(data_train.data))\n",
    "train_bigram = bigram_vectorizer_memory_inefficient.fit_transform(get_tokens(data_train.data))\n",
    "print('%i vs %i' % train_bigram_df_ge_3.shape, train_bigram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's improve upon simple counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = transformer.fit_transform(train_unigram.toarray())\n",
    "tfidf_test = transformer.transform(test_unigram.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.23571696, 0.        ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train[0:3,0:10].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tfidf matrix has shape 1197 x 18569.\n",
      "There are 18569 many word forms distinguished.\n"
     ]
    }
   ],
   "source": [
    "print(\"The tfidf matrix has shape %i x %i.\" % tfidf_train.shape)\n",
    "print(\"There are %i many word forms distinguished.\" % len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_reduced = svd.fit_transform(tfidf_train.T)\n",
    "tfidf_train_reduced.shape\n",
    "\n",
    "word_to_id = {word:i for i, word in enumerate(vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_words = ['stick', 'puck', 'homerun']\n",
    "query_word_ids = [word_to_id[w] for w in query_words]\n",
    "query_vecs = tfidf_train_reduced[query_word_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Write a method which calculates the cosine similarity between two vectors.\n",
    "1. Write a method which given a matrix of vectors and a given vector returns the index of the most similar vector from the matrix according to the cosine similarity. (Do not feel bad about writing a non-optimized code for the moment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine(u, v):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_most_similar(X, i):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stick stick None\n",
      "stick puck None\n",
      "stick homerun None\n",
      "puck stick None\n",
      "puck puck None\n",
      "puck homerun None\n",
      "homerun stick None\n",
      "homerun puck None\n",
      "homerun homerun None\n"
     ]
    }
   ],
   "source": [
    "for u_word, u_vec in zip(query_words, query_vecs):\n",
    "    for v_word, v_vec in zip(query_words, query_vecs):\n",
    "        print(u_word, v_word, calc_cosine(u_vec, v_vec))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
